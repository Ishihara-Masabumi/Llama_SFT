"""
Evaluate SFT model (before DPO) vs DPO model (after DPO).
- Qualitative: side-by-side generation comparison on Japanese prompts
- Quantitative: DPO reward accuracy on held-out eval set
"""

import json
import sys
import time
import torch
from peft import PeftModel
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

EVAL_PROMPTS = [
    "æ—¥æœ¬ã®å››å­£ã®ä¸­ã§ã€ã‚ãªãŸãŒæœ€ã‚‚å¥½ããªå­£ç¯€ã¨ãã®ç†ç”±ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã«ã€Pythonã‚’å­¦ã¶ãƒ¡ãƒªãƒƒãƒˆã‚’3ã¤æŒ™ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    "å¥åº·çš„ãªé£Ÿç”Ÿæ´»ã‚’é€ã‚‹ãŸã‚ã«ã€æ¯æ—¥å¿ƒãŒã‘ã‚‹ã¹ãã“ã¨ã‚’å…·ä½“çš„ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„ã€‚",
    "AIãŒç¤¾ä¼šã«ã‚‚ãŸã‚‰ã™ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã«ã¤ã„ã¦ã€ãƒãƒ©ãƒ³ã‚¹ã‚ˆãè«–ã˜ã¦ãã ã•ã„ã€‚",
    "æ¬¡ã®æ–‡ç« ã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ï¼šã€Œæ¡œã®å­£ç¯€ã«ãªã‚‹ã¨ã€å¤šãã®äººãŒå…¬åœ’ã§ãŠèŠ±è¦‹ã‚’æ¥½ã—ã¿ã¾ã™ã€‚ã€",
    "å°å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«ã€åœ°çƒæ¸©æš–åŒ–ã®åŸå› ã¨å½±éŸ¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    "åŠ¹ç‡çš„ãªæ™‚é–“ç®¡ç†ã®æ–¹æ³•ã‚’5ã¤ã€å…·ä½“çš„ãªä¾‹ã‚’äº¤ãˆã¦ææ¡ˆã—ã¦ãã ã•ã„ã€‚",
    "ã€ŒåŠªåŠ›ã¯å¿…ãšå ±ã‚ã‚Œã‚‹ã€ã¨ã„ã†è€ƒãˆæ–¹ã«å¯¾ã—ã¦ã€è³›æˆãƒ»åå¯¾ã®ä¸¡æ–¹ã®ç«‹å ´ã‹ã‚‰æ„è¦‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚",
]

SFT_MODEL_PATH = "/home/ubuntu/Llama_SFT/sft_merged_llama3"
DPO_ADAPTER_PATH = "/home/ubuntu/Llama_SFT/results_dpo_simpo"


def load_model(model_path, adapter_path=None):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda",
    )
    if adapter_path:
        model = PeftModel.from_pretrained(model, adapter_path)
        model = model.merge_and_unload()
    model.eval()
    return model, tokenizer


def generate_response(model, tokenizer, prompt, max_new_tokens=512):
    messages = [{"role": "user", "content": prompt}]
    input_text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1,
        )
    response = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True
    )
    return response.strip()


def compute_dpo_eval_metrics(model, ref_model, tokenizer, num_samples=200):
    """Compute reward accuracy on held-out DPO eval data."""
    dataset = load_dataset(
        "Aratako/iterative-dpo-data-for-SimPO-iter2", split="train"
    )
    split = dataset.train_test_split(test_size=0.05, seed=42)
    eval_data = split["test"].select(range(min(num_samples, len(split["test"]))))

    correct = 0
    total = 0

    for row in eval_data:
        messages_chosen = [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": row["chosen"]},
        ]
        messages_rejected = [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": row["rejected"]},
        ]

        text_chosen = tokenizer.apply_chat_template(
            messages_chosen, tokenize=False
        )
        text_rejected = tokenizer.apply_chat_template(
            messages_rejected, tokenize=False
        )

        for text, label in [(text_chosen, "chosen"), (text_rejected, "rejected")]:
            inputs = tokenizer(
                text, return_tensors="pt", truncation=True, max_length=2048
            ).to(model.device)
            with torch.no_grad():
                logits = model(**inputs).logits
                ref_logits = ref_model(**inputs).logits

            # Compute log probs
            shift_logits = logits[:, :-1, :]
            shift_labels = inputs["input_ids"][:, 1:]
            log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)

            ref_shift_logits = ref_logits[:, :-1, :]
            ref_log_probs = torch.nn.functional.log_softmax(ref_shift_logits, dim=-1)
            ref_token_log_probs = ref_log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)

            if label == "chosen":
                chosen_reward = (token_log_probs.sum() - ref_token_log_probs.sum()).item()
            else:
                rejected_reward = (token_log_probs.sum() - ref_token_log_probs.sum()).item()

        if chosen_reward > rejected_reward:
            correct += 1
        total += 1

        if total % 50 == 0:
            print(f"  Eval progress: {total}/{len(eval_data)}, "
                  f"accuracy so far: {correct/total:.4f}")

    return correct / total if total > 0 else 0.0


def main():
    print("=" * 80)
    print("DPOå­¦ç¿’ å‰å¾Œã®è©•ä¾¡")
    print("=" * 80)

    # --- Part 1: Qualitative evaluation (generation comparison) ---
    print("\n[1/2] ç”Ÿæˆæ¯”è¼ƒ (Qualitative Evaluation)")
    print("-" * 80)

    results = []

    # SFT model
    print("\nSFTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    model_sft, tokenizer = load_model(SFT_MODEL_PATH)

    print("SFTãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆä¸­...")
    sft_responses = []
    for i, prompt in enumerate(EVAL_PROMPTS):
        print(f"  Prompt {i+1}/{len(EVAL_PROMPTS)}...")
        resp = generate_response(model_sft, tokenizer, prompt)
        sft_responses.append(resp)

    # Keep SFT model as ref, load DPO on top
    ref_model = model_sft  # will be used for metrics later

    print("\nDPOãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    model_dpo, _ = load_model(SFT_MODEL_PATH, DPO_ADAPTER_PATH)

    print("DPOãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆä¸­...")
    dpo_responses = []
    for i, prompt in enumerate(EVAL_PROMPTS):
        print(f"  Prompt {i+1}/{len(EVAL_PROMPTS)}...")
        resp = generate_response(model_dpo, tokenizer, prompt)
        dpo_responses.append(resp)

    # Print results
    print("\n" + "=" * 80)
    print("ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
    print("=" * 80)

    for i, prompt in enumerate(EVAL_PROMPTS):
        print(f"\n{'â”€' * 80}")
        print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+1}: {prompt}")
        print(f"{'â”€' * 80}")
        print(f"\nã€SFTãƒ¢ãƒ‡ãƒ« (DPOå‰)ã€‘")
        print(sft_responses[i][:500])
        if len(sft_responses[i]) > 500:
            print(f"... (å…¨{len(sft_responses[i])}æ–‡å­—)")
        print(f"\nã€DPOãƒ¢ãƒ‡ãƒ« (DPOå¾Œ)ã€‘")
        print(dpo_responses[i][:500])
        if len(dpo_responses[i]) > 500:
            print(f"... (å…¨{len(dpo_responses[i])}æ–‡å­—)")

        results.append({
            "prompt": prompt,
            "sft_response": sft_responses[i],
            "dpo_response": dpo_responses[i],
            "sft_length": len(sft_responses[i]),
            "dpo_length": len(dpo_responses[i]),
        })

    # --- Part 2: Quantitative evaluation (reward accuracy) ---
    print("\n" + "=" * 80)
    print("[2/2] å®šé‡è©•ä¾¡ (Reward Accuracy on Eval Set)")
    print("-" * 80)

    # Need both models on GPU - check memory
    # ref_model is SFT (already on GPU), model_dpo is DPO (already on GPU)
    # This won't work with both on GPU for 3B model - evaluate sequentially

    # Move DPO model off GPU, compute SFT reward accuracy
    del model_dpo
    torch.cuda.empty_cache()

    print("\nSFTãƒ¢ãƒ‡ãƒ«ã®reward accuracyè¨ˆç®—ä¸­...")
    # For SFT model, reward relative to itself is always 0, so accuracy = random
    # Instead, compute perplexity on chosen vs rejected
    dataset = load_dataset(
        "Aratako/iterative-dpo-data-for-SimPO-iter2", split="train"
    )
    split = dataset.train_test_split(test_size=0.05, seed=42)
    eval_data = split["test"].select(range(min(200, len(split["test"]))))

    sft_chosen_better = 0
    sft_total = 0
    for row in eval_data:
        messages_chosen = [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": row["chosen"]},
        ]
        messages_rejected = [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": row["rejected"]},
        ]
        text_chosen = tokenizer.apply_chat_template(messages_chosen, tokenize=False)
        text_rejected = tokenizer.apply_chat_template(messages_rejected, tokenize=False)

        chosen_ll = 0
        rejected_ll = 0
        for text, label in [(text_chosen, "chosen"), (text_rejected, "rejected")]:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048).to(ref_model.device)
            with torch.no_grad():
                logits = ref_model(**inputs).logits
            shift_logits = logits[:, :-1, :]
            shift_labels = inputs["input_ids"][:, 1:]
            log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)
            avg_ll = token_log_probs.mean().item()
            if label == "chosen":
                chosen_ll = avg_ll
            else:
                rejected_ll = avg_ll

        if chosen_ll > rejected_ll:
            sft_chosen_better += 1
        sft_total += 1
        if sft_total % 50 == 0:
            print(f"  Progress: {sft_total}/{len(eval_data)}, "
                  f"accuracy: {sft_chosen_better/sft_total:.4f}")

    sft_accuracy = sft_chosen_better / sft_total

    # Now DPO model
    del ref_model
    torch.cuda.empty_cache()

    print("\nDPOãƒ¢ãƒ‡ãƒ«ã®reward accuracyè¨ˆç®—ä¸­...")
    model_dpo, _ = load_model(SFT_MODEL_PATH, DPO_ADAPTER_PATH)

    dpo_chosen_better = 0
    dpo_total = 0
    for row in eval_data:
        messages_chosen = [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": row["chosen"]},
        ]
        messages_rejected = [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": row["rejected"]},
        ]
        text_chosen = tokenizer.apply_chat_template(messages_chosen, tokenize=False)
        text_rejected = tokenizer.apply_chat_template(messages_rejected, tokenize=False)

        chosen_ll = 0
        rejected_ll = 0
        for text, label in [(text_chosen, "chosen"), (text_rejected, "rejected")]:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048).to(model_dpo.device)
            with torch.no_grad():
                logits = model_dpo(**inputs).logits
            shift_logits = logits[:, :-1, :]
            shift_labels = inputs["input_ids"][:, 1:]
            log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)
            avg_ll = token_log_probs.mean().item()
            if label == "chosen":
                chosen_ll = avg_ll
            else:
                rejected_ll = avg_ll

        if chosen_ll > rejected_ll:
            dpo_chosen_better += 1
        dpo_total += 1
        if dpo_total % 50 == 0:
            print(f"  Progress: {dpo_total}/{len(eval_data)}, "
                  f"accuracy: {dpo_chosen_better/dpo_total:.4f}")

    dpo_accuracy = dpo_chosen_better / dpo_total

    # Summary
    print("\n" + "=" * 80)
    print("è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)

    print(f"\n{'æŒ‡æ¨™':<30} {'SFTãƒ¢ãƒ‡ãƒ«(DPOå‰)':<20} {'DPOãƒ¢ãƒ‡ãƒ«(DPOå¾Œ)':<20}")
    print(f"{'â”€' * 70}")
    print(f"{'Chosené¸å¥½ç²¾åº¦ (eval 200ä»¶)':<30} {sft_accuracy:<20.4f} {dpo_accuracy:<20.4f}")

    avg_sft_len = sum(r["sft_length"] for r in results) / len(results)
    avg_dpo_len = sum(r["dpo_length"] for r in results) / len(results)
    print(f"{'å¹³å‡å¿œç­”é•· (æ–‡å­—æ•°)':<30} {avg_sft_len:<20.1f} {avg_dpo_len:<20.1f}")

    # Save full results
    output_path = "/home/ubuntu/Llama_SFT/eval_results.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "sft_chosen_accuracy": sft_accuracy,
            "dpo_chosen_accuracy": dpo_accuracy,
            "avg_sft_response_length": avg_sft_len,
            "avg_dpo_response_length": avg_dpo_len,
            "generation_comparisons": results,
        }, f, ensure_ascii=False, indent=2)
    print(f"\nè©³ç´°çµæœã‚’ä¿å­˜: {output_path}")

    del model_dpo
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
